{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95279a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install xgboost\n",
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install -U sentence-transformers\n",
    "!pip install torch_explain\n",
    "!pip install torch\n",
    "#!pip -q install langchain huggingface_hub transformers sentence_transformers\n",
    "!pip install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa0ffcb-6710-41ea-944e-4765f2a2fd2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertForTokenClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from torch_explain.nn.concepts import ConceptReasoningLayer, ConceptEmbedding\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from typing import Dict, List\n",
    "from utilities import *\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb8b5c-13ae-4ce5-8c64-d868ca37850a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93b87a-2c27-4653-b8fe-d5007a69a642",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03c88935-441e-4a74-b701-b86921ca2fac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# depression \n",
    "class Args():\n",
    "    max_length = 512\n",
    "    dataset = 'depression'\n",
    "    pseudo_labeling = 'mixtral'\n",
    "    configuration = 'cbm_fc'\n",
    "    backbone = 'bert'\n",
    "    model_name =  None   # Big: 'all-mpnet-base-v2'  Medium: 'all-MiniLM-L12-v2'  Small: 'all-MiniLM-L6-v2'\n",
    "    chuncks = None \n",
    "    sentences_per_concept = 1\n",
    "    n_concepts = 6\n",
    "    lr = 5e-6\n",
    "    epochs = 40\n",
    "    step_size = 10\n",
    "    gamma = 0.1\n",
    "    n_labels = 2\n",
    "    concept_size = 768\n",
    "    seed = 42\n",
    "    N = None\n",
    "    threshold_optimization = False\n",
    "    lambda_coeff = 0.5\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942c3e5-959c-4d6e-aa37-53bf6d60954e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cebab\n",
    "class Args():\n",
    "    max_length = 100\n",
    "    dataset = 'cebab'\n",
    "    pseudo_labeling = 'mixtral'\n",
    "    configuration = 'modified_cem' # e modified_dcl, supervised_dcl\n",
    "    backbone = 'mixtral'\n",
    "    model_name =  None   # Big: 'all-mpnet-base-v2'  Medium: 'all-MiniLM-L12-v2'  Small: 'all-MiniLM-L6-v2'\n",
    "    chuncks = None \n",
    "    sentences_per_concept = 1\n",
    "    n_concepts = 4\n",
    "    lr = 1e-6\n",
    "    epochs = 50\n",
    "    step_size = 10\n",
    "    gamma = 0.1\n",
    "    n_labels = 2\n",
    "    concept_size = 16\n",
    "    seed = 42\n",
    "    N = None\n",
    "    threshold_optimization = False\n",
    "    lambda_coeff = 0.5\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a03e2b9-1e8e-462c-b474-b89c05a2ba44",
   "metadata": {
    "tags": []
   },
   "source": [
    "# drug\n",
    "class Args():\n",
    "    max_length = 256\n",
    "    dataset = 'drug'\n",
    "    pseudo_labeling = 'mixtral'\n",
    "    # Big: 'all-mpnet-base-v2'  Small: 'all-MiniLM-L6-v2'   Medium: 'all-MiniLM-L12-v2' \n",
    "    model_name =  None\n",
    "    configuration = 'XGBoost'\n",
    "    backbone = 'bert'\n",
    "    chuncks = False # True\n",
    "    sentences_per_concept = 1 #3\n",
    "    n_concepts = 2\n",
    "    lr = 5e-6\n",
    "    epochs = 100\n",
    "    step_size = 100\n",
    "    gamma = 0.1\n",
    "    n_labels = 3\n",
    "    concept_size = 768\n",
    "    seed = 42\n",
    "    N = 100 \n",
    "    threshold_optimization = False\n",
    "    lambda_coeff = 0.5\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c32fb6e-c364-4657-94f3-8f7fcde6edcc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### emo\n",
    "class Args():\n",
    "    max_length = 256\n",
    "    dataset = 'emo'\n",
    "    pseudo_labeling = 'mixtral'\n",
    "    model_name =  None\n",
    "    configuration = 'dcr'\n",
    "    backbone = 'bert'\n",
    "    chuncks = False # True\n",
    "    sentences_per_concept = 1 #3\n",
    "    N = 100 #50\n",
    "    n_concepts = 4\n",
    "    lr = 5e-6\n",
    "    epochs = 100\n",
    "    step_size = 10\n",
    "    gamma = 0.1\n",
    "    n_labels = 3\n",
    "    concept_size = 768\n",
    "    seed = 42\n",
    "    threshold_optimization = False\n",
    "    lambda_coeff = 0.5\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe35d3-3e81-4f13-9a98-bf434122a9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = args.max_length\n",
    "dataset = args.dataset\n",
    "pseudo_labeling = args.pseudo_labeling\n",
    "model_name =  args.model_name   \n",
    "configuration = args.configuration\n",
    "chuncks = args.chuncks\n",
    "sentences_per_concept = args.sentences_per_concept\n",
    "N = args.N\n",
    "n_concepts = args.n_concepts\n",
    "threshold_optimization = args.threshold_optimization\n",
    "labeler_tokenizer = None\n",
    "n_labels = args.n_labels\n",
    "concept_size = args.concept_size\n",
    "backbone = args.backbone\n",
    "lambda_coeff = args.lambda_coeff\n",
    "\n",
    "if pseudo_labeling == 'mistral':\n",
    "    if model_name==None:\n",
    "        model_name = 'mistralai/Mistral-7B-v0.1'\n",
    "    labeler_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "elif pseudo_labeling == 'mixtral':\n",
    "    #model_name = 'mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-metaoffload-HQQ'\n",
    "    #from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    if model_name==None:\n",
    "        model_name = 'mistralai/Mixtral-8x7B-v0.1'\n",
    "    labeler_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", model_max_length = max_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b29db5-b35e-4e0d-9c14-6dd11cc1d8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = f'concept_sentences/{dataset}/'\n",
    "\n",
    "def read_from_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "if dataset=='depression':\n",
    "    concepts = [['',''],['',''],['',''],['',''],['',''],['','']]\n",
    "else:\n",
    "    concepts = read_from_json_file(file_path+f'{sentences_per_concept}_sentenceXconcept.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2431c29-bcc9-4cb7-84b3-57282cd0f7ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if pseudo_labeling == 'sbert':\n",
    "    loaded_set_folder = f'loaded_sets/{dataset}/{pseudo_labeling}_{model_name}_{chuncks}_{str(sentences_per_concept)}/'\n",
    "else:\n",
    "    loaded_set_folder = f'loaded_sets/{dataset}/{pseudo_labeling}/'\n",
    "\n",
    "if not os.path.exists(loaded_set_folder):\n",
    "    os.makedirs(loaded_set_folder)\n",
    "    print('Path created!')\n",
    "\n",
    "if len([x for x in os.listdir(loaded_set_folder) if 'ipynb_checkpoints' not in x])==0:\n",
    "\n",
    "    if dataset=='cebab':\n",
    "        loader = Cebab_loader('datasets/cebab', concepts, sentences_per_concept, chuncks, 'cebab_train.csv', \n",
    "                              'cebab_validation.csv', 'cebab_test.csv', max_length, tokenizer, \n",
    "                              pseudo_labeling, model_name, labeler_tokenizer)\n",
    "        loader.load()\n",
    "        loaded_train, loaded_val, loaded_test = loader.collator(batch_train=64, batch_val_test=32)\n",
    "\n",
    "    elif dataset=='drug':\n",
    "        loader = Drug_loader('datasets/drug/drug_train.tsv', 'datasets/drug/drug_test.tsv', concepts, sentences_per_concept, chuncks, max_length, tokenizer, \n",
    "                              pseudo_labeling, model_name, labeler_tokenizer, args.seed)  \n",
    "        loader.load()\n",
    "        loaded_train, loaded_val, loaded_test = loader.collator(batch_train=64, batch_val_test=32)\n",
    "\n",
    "    elif dataset=='emo':\n",
    "        loader = Emo_loader('datasets/emo/MultiEmotions-It.tsv', concepts, sentences_per_concept, chuncks, max_length, tokenizer, \n",
    "                              pseudo_labeling, model_name, labeler_tokenizer, args.seed)            \n",
    "        loader.load()\n",
    "        loaded_train, loaded_val, loaded_test = loader.collator(batch_train=64, batch_val_test=32)\n",
    "    elif dataset=='depression':\n",
    "        loader = depressed_loader('datasets/depression', concepts, sentences_per_concept, chuncks, max_length, tokenizer, \n",
    "                              pseudo_labeling, model_name, labeler_tokenizer, args.seed)            \n",
    "        loader.load()\n",
    "        loaded_train, loaded_val, loaded_test = loader.collator(batch_train=64, batch_val_test=32)\n",
    "\n",
    "    with open(f'{loaded_set_folder}/loaded_train.pkl', 'wb') as f:\n",
    "        pickle.dump(loaded_train, f)\n",
    "    with open(f'{loaded_set_folder}/loaded_val.pkl', 'wb') as f:\n",
    "        pickle.dump(loaded_val, f)\n",
    "    with open(f'{loaded_set_folder}/loaded_test.pkl', 'wb') as f:\n",
    "        pickle.dump(loaded_test, f)\n",
    "else:\n",
    "    with open(f'{loaded_set_folder}/loaded_train.pkl', 'rb') as f:\n",
    "        loaded_train = pickle.load(f)\n",
    "    with open(f'{loaded_set_folder}/loaded_val.pkl', 'rb') as f:\n",
    "        loaded_val = pickle.load(f)\n",
    "    with open(f'{loaded_set_folder}/loaded_test.pkl', 'rb') as f:\n",
    "        loaded_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7838bc-7746-4026-a563-bb42301a6c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnt=0\n",
    "for lemme in loaded_train:\n",
    "    cnt += lemme['embedded_reviews'].shape[0]\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214db556-9add-42d7-acf1-3caeea29f467",
   "metadata": {},
   "source": [
    "# Evaluate quality of unsupervised concept discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa583378-6e61-4491-9aa0-24147ec80a59",
   "metadata": {},
   "source": [
    "# Threshold selection\n",
    "N samples are extracted from the validation set and the the threhsold that maximizes \n",
    "the f1 score macro over the different concepts is selected. When we are dealing with mistral/mixtral classification this operation\n",
    "doesn't change the annotation performance since those annotators classify using only 1 (concept present) or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62287bdc-5670-46ad-92f3-d19b558ff699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if pseudo_labeling == 'sbert' and threshold_optimization==True:\n",
    "    # The concepts predictions for the whole validation are collected\n",
    "    true_concepts = torch.zeros(1, n_concepts)\n",
    "    predicted_concepts = torch.zeros(1, n_concepts, 2)\n",
    "    for sentence_batch in loaded_val:\n",
    "        if dataset=='cebab':\n",
    "            food = sentence_batch['food']\n",
    "            ambiance = sentence_batch['ambiance']\n",
    "            service = sentence_batch['service']\n",
    "            noise = sentence_batch['noise']\n",
    "            true_concepts = torch.cat([true_concepts, torch.hstack([food.unsqueeze(1), ambiance.unsqueeze(1), service.unsqueeze(1), noise.unsqueeze(1)])])  \n",
    "        elif dataset=='drug':\n",
    "            effectiveness = sentence_batch['effectiveness']\n",
    "            sideEffects = sentence_batch['sideEffects']\n",
    "            true_concepts = torch.cat([true_concepts, torch.hstack([effectiveness.unsqueeze(1), sideEffects.unsqueeze(1)])])  \n",
    "        elif dataset=='emo':\n",
    "            gioia = sentence_batch['joy']\n",
    "            fiducia = sentence_batch['trust']\n",
    "            tristezza = sentence_batch['sadness']\n",
    "            sorpresa = sentence_batch['surprise']\n",
    "            true_concepts = torch.cat([true_concepts, torch.hstack([gioia.unsqueeze(1), fiducia.unsqueeze(1), tristezza.unsqueeze(1), sorpresa.unsqueeze(1)])])         \n",
    "        unsupervised_concepts = sentence_batch['concept_score']\n",
    "        predicted_concepts = torch.cat([predicted_concepts, unsupervised_concepts])\n",
    "\n",
    "    true_concepts = true_concepts[1:,:]\n",
    "    predicted_concepts = predicted_concepts[1:,:]\n",
    "    \n",
    "    # From the validation N samples are sampled in order to set the threshold\n",
    "    p = torch.ones(len(loaded_val.dataset))/len(loaded_val.dataset)\n",
    "    index = p.multinomial(num_samples=N, replacement=True)\n",
    "    #predicted_concepts[index,:,:].shape, true_concepts[index,:].shape\n",
    "    thrs = np.linspace(0,1,100)\n",
    "    best_f1 = 0\n",
    "    for thr in thrs:\n",
    "        if dataset=='cebab':\n",
    "            preds = torch.where(F.softmax(torch.where(predicted_concepts[index,:,:]>thr,predicted_concepts[index,:,:],0), dim=-1)[:,:,1]>0.5, 1, 0)\n",
    "        elif dataset=='drug':\n",
    "            preds = torch.argmax(predicted_concepts[index,:,:], dim=-1)\n",
    "            #preds = torch.where(predicted_concepts[index,:,1]>thr, 1, 0) #torch.where(F.softmax(predicted_concepts, dim=-1)[index,:,1]>thr, 1, 0) \n",
    "        elif dataset=='emo':\n",
    "            preds = torch.where(predicted_concepts[index,:,1]>thr, 1, 0) \n",
    "        f1s = []\n",
    "        for i in range(n_concepts):\n",
    "            f1s.append(classification_report(true_concepts[index,i], preds[:,i].detach().cpu().numpy(), output_dict=True, zero_division=0)['macro avg']['f1-score'])\n",
    "        if np.array(f1s).mean()>best_f1:\n",
    "            threshold = thr\n",
    "            best_f1 = np.array(f1s).mean()\n",
    "    print(f'Result obtined by sampling and \"manually\" labeling {N} samples from the validation-set.')\n",
    "    print('Best threshold:', threshold, '; best f1-score macro:', best_f1)\n",
    "else:\n",
    "    threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fcdf76-aa76-4ee0-8fa2-a1be9d25caa1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a30c7-4ac6-4db0-b56f-705b7834aa30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_folder = f\"results/{dataset}/{backbone}/\"\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "cnt = len([x for x in os.listdir(results_folder) if 'ipynb' not in x])\n",
    "if pseudo_labeling == 'sbert':\n",
    "    result_folder = results_folder+f'{configuration}_{model_name}_E{cnt}/'\n",
    "else:\n",
    "    result_folder = results_folder+f'{configuration}_E{cnt}/'\n",
    "    \n",
    "os.makedirs(result_folder)\n",
    "result_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b4ba16-c192-45e5-80c5-b7ae81b9771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset != 'depression':\n",
    "    true_concepts = torch.zeros(1, n_concepts)\n",
    "    predicted_concepts = torch.zeros(1, n_concepts)\n",
    "    for sentence_batch in loaded_test:\n",
    "        if dataset=='cebab':\n",
    "            food = sentence_batch['food']\n",
    "            ambiance = sentence_batch['ambiance']\n",
    "            service = sentence_batch['service']\n",
    "            noise = sentence_batch['noise']\n",
    "            true_concepts = torch.cat([true_concepts, torch.hstack([food.unsqueeze(1), ambiance.unsqueeze(1), service.unsqueeze(1), noise.unsqueeze(1)])])  \n",
    "        elif dataset=='drug':\n",
    "            effectiveness = sentence_batch['effectiveness']\n",
    "            sideEffects = sentence_batch['sideEffects']\n",
    "            true_concepts = torch.cat([true_concepts, torch.hstack([effectiveness.unsqueeze(1), sideEffects.unsqueeze(1)])]) \n",
    "        elif dataset=='emo':\n",
    "            gioia = sentence_batch['joy']\n",
    "            fiducia = sentence_batch['trust']\n",
    "            tristezza = sentence_batch['sadness']\n",
    "            sorpresa = sentence_batch['surprise']\n",
    "            true_concepts = torch.cat([true_concepts, torch.hstack([gioia.unsqueeze(1), fiducia.unsqueeze(1), tristezza.unsqueeze(1), sorpresa.unsqueeze(1)])])         \n",
    "\n",
    "        if pseudo_labeling=='sbert' and dataset=='cebab':\n",
    "            unsupervised_concepts = torch.where(F.softmax(torch.where(sentence_batch['concept_score']>threshold,sentence_batch['concept_score'],0), dim=-1)[:,:,1]>0.5, 1, 0)\n",
    "        elif pseudo_labeling=='sbert' and dataset=='drug':\n",
    "            unsupervised_concepts = torch.argmax(sentence_batch['concept_score'], dim=-1) #torch.where(sentence_batch['concept_score'][:,:,1]>threshold, 1, 0) \n",
    "        elif pseudo_labeling=='sbert' and dataset=='emo':\n",
    "            unsupervised_concepts = torch.where(sentence_batch['concept_score'][:,:,1]>threshold, 1, 0) \n",
    "        elif pseudo_labeling in ['mistral', 'mixtral']:\n",
    "            unsupervised_concepts = sentence_batch['concept_score']\n",
    "\n",
    "        predicted_concepts = torch.cat([predicted_concepts, unsupervised_concepts])\n",
    "\n",
    "    true_concepts = true_concepts[1:,:]\n",
    "    predicted_concepts = predicted_concepts[1:,:]\n",
    "\n",
    "    fig, ax = plt.subplots(1, n_concepts)\n",
    "    fig.set_size_inches(20,5)\n",
    "    if dataset=='cebab':\n",
    "        names = ['good_food', 'good_ambiance', 'good_service', 'good_noise']\n",
    "    elif dataset=='drug':\n",
    "        names = ['effectiveness', 'sideEffects']\n",
    "    elif dataset=='emo':\n",
    "        names = ['joy', 'trust', 'sadness', 'surprise']\n",
    "    for i in range(n_concepts):\n",
    "        print()\n",
    "        cm = confusion_matrix(true_concepts[:,i], predicted_concepts[:,i].to(torch.long))\n",
    "        sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', ax=ax[i])\n",
    "        ax[i].set_xlabel(\"True\")\n",
    "        ax[i].set_ylabel(\"Pred\")\n",
    "\n",
    "        cr = pd.DataFrame(classification_report(true_concepts[:,i], predicted_concepts[:,i].detach().cpu().numpy(), output_dict=True)).T\n",
    "        cr.index = [\"0\", \"1\", \"accuracy\", \"macro avg\", \"weighted avg\"]\n",
    "        if configuration in ['cbm_fc_ct', 'cbm_ff_ct', 'modified_cem', 'DTree_ct', 'XGBoost_ct', 'cem_ct']:\n",
    "            if not os.path.exists(result_folder+'/concepts'):\n",
    "                os.makedirs(result_folder+'/concepts')\n",
    "            cr.to_csv(result_folder+f'/concepts/{names[i]}_classification_report.csv', index=True)\n",
    "\n",
    "        # store the results on the folder related to the labeling strategy\n",
    "        if not os.path.exists(loaded_set_folder+f'/concepts'):\n",
    "            os.makedirs(loaded_set_folder+f'/concepts')\n",
    "        cr.to_csv(loaded_set_folder+f'/concepts/{names[i]}_classification_report.csv', index=True)\n",
    "\n",
    "        ax[i].set_title(names[i]+'; f1-score macro: '+str(round(classification_report(true_concepts[:,i], predicted_concepts[:,i].to(torch.long), output_dict=True)['macro avg']['f1-score'],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98432528-46d2-445d-b2f7-a3875bf13489",
   "metadata": {},
   "source": [
    "# Modules definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca10c86-bad3-4967-b0ab-f96387f4bd97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if backbone=='mixtral':\n",
    "    embedding_size = 4096\n",
    "elif backbone=='bert':\n",
    "    embedding_size = 768\n",
    "\n",
    "# Combine the layers in a sequential model\n",
    "if configuration in ['dcr', 'supervised_dcr']:\n",
    "    cem = ConceptEmbedding(embedding_size, n_concepts, concept_size).to(device)\n",
    "    dcr = ConceptReasoningLayer(concept_size, n_labels, temperature=0.1).to(device)\n",
    "    if backbone=='bert':\n",
    "        concept_encoder = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        model = torch.nn.Sequential(concept_encoder, cem, dcr)\n",
    "    else:\n",
    "        model = torch.nn.Sequential(cem, dcr)\n",
    "elif configuration=='cem':\n",
    "    cem = ConceptEmbedding(embedding_size, n_concepts, concept_size).to(device)\n",
    "    ff = nn.Sequential(nn.Linear(n_concepts*concept_size, 10), nn.ReLU(), nn.Linear(10, n_labels), nn.Softmax()).to(device)\n",
    "    if backbone=='bert':\n",
    "        concept_encoder = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        model = torch.nn.Sequential(concept_encoder, cem, ff)\n",
    "    else:\n",
    "        model = torch.nn.Sequential(cem, ff)\n",
    "elif configuration=='cem_ct':\n",
    "    cem = Modified_cem(embedding_size, n_concepts, concept_size).to(device)\n",
    "    ff = nn.Sequential(nn.Linear(n_concepts*concept_size, 10), nn.ReLU(), nn.Linear(10, n_labels), nn.Softmax()).to(device)\n",
    "    if backbone=='bert':\n",
    "        concept_encoder = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        model = torch.nn.Sequential(concept_encoder, cem, ff)\n",
    "    else:\n",
    "        model = torch.nn.Sequential(cem, ff)   \n",
    "elif configuration=='cbm_fc':\n",
    "    ff_concept = nn.Sequential(nn.Linear(embedding_size, n_concepts), nn.Sigmoid()).to(device)\n",
    "    fc_task = nn.Sequential(nn.Linear(n_concepts, n_labels), nn.Softmax()).to(device)\n",
    "    if backbone=='bert':\n",
    "        concept_encoder = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        model = torch.nn.Sequential(concept_encoder, ff_concept, fc_task)\n",
    "    else:\n",
    "        model = torch.nn.Sequential(ff_concept, fc_task)\n",
    "elif configuration=='cbm_ff':\n",
    "    concept_encoder = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "    ff_concept = nn.Sequential(nn.Linear(embedding_size, n_concepts), nn.Sigmoid()).to(device)\n",
    "    ff_task = nn.Sequential(nn.Linear(n_concepts, 3*n_concepts), nn.ReLU(), nn.Linear(3*n_concepts, n_labels), nn.Softmax()).to(device)\n",
    "    if backbone=='bert':\n",
    "        concept_encoder = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        model = torch.nn.Sequential(concept_encoder, ff_concept, ff_task)\n",
    "    else:\n",
    "        model = torch.nn.Sequential(ff_concept, ff_task) \n",
    "elif configuration=='modified_cem':\n",
    "    cem = Modified_cem(embedding_size, n_concepts, concept_size).to(device)\n",
    "    dcr = ConceptReasoningLayer(concept_size, n_labels, temperature=0.1).to(device)\n",
    "    if backbone=='bert':\n",
    "        concept_encoder = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        model = torch.nn.Sequential(concept_encoder, cem, dcr)\n",
    "    else:\n",
    "        model = torch.nn.Sequential(cem, dcr)\n",
    "elif configuration in ['dcl', 'supervised_dcl']:\n",
    "    cem = ConceptEmbedding(embedding_size, n_concepts, concept_size).to(device)\n",
    "    dcl = ConceptLinearLayer(concept_size, n_labels, bias=True, attention=False).to(device)\n",
    "    if backbone=='bert':\n",
    "        concept_encoder = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        model = torch.nn.Sequential(concept_encoder, cem, dcl)\n",
    "    else:\n",
    "        model = torch.nn.Sequential(cem, dcl)\n",
    "elif configuration=='modified_dcl':\n",
    "    cem = Modified_cem(embedding_size, n_concepts, concept_size).to(device)\n",
    "    dcl = ConceptLinearLayer(concept_size, n_labels, bias=True, attention=False).to(device)\n",
    "    if backbone=='bert':\n",
    "        concept_encoder = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        model = torch.nn.Sequential(concept_encoder, cem, dcl)\n",
    "    else:\n",
    "        model = torch.nn.Sequential(cem, dcl)\n",
    "elif configuration=='cbm_fc_ct':\n",
    "    fc_task = nn.Sequential(nn.Linear(n_concepts, n_labels), nn.Softmax()).to(device)\n",
    "    model = nn.Sequential(fc_task)\n",
    "elif configuration=='cbm_ff_ct':\n",
    "    ff_task = nn.Sequential(nn.Linear(n_concepts, 3*n_concepts), nn.ReLU(), nn.Linear(3*n_concepts, n_labels), nn.Softmax()).to(device)\n",
    "    model = nn.Sequential(ff_task)\n",
    "elif configuration=='DTree':\n",
    "    ff_concept = nn.Sequential(nn.Linear(embedding_size, n_concepts), nn.Sigmoid()).to(device)\n",
    "    if backbone=='bert':\n",
    "        concept_encoder = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        model = torch.nn.Sequential(concept_encoder, ff_concept)\n",
    "    else:\n",
    "        model = torch.nn.Sequential(ff_concept)\n",
    "    dtree = DecisionTreeClassifier()\n",
    "elif configuration=='DTree_ct':\n",
    "    dtree = DecisionTreeClassifier()\n",
    "elif configuration=='XGBoost':\n",
    "    # Set XGBoost parameters\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',  # Multiclass classification\n",
    "        'num_class': n_labels,                 # Number of classes\n",
    "        'max_depth': 3,                 # Maximum depth of each tree\n",
    "        'eta': 0.3,                     # Learning rate\n",
    "        'eval_metric': 'merror'         # Evaluation metric\n",
    "    }\n",
    "    # Train the XGBoost model\n",
    "    num_rounds = 5  # Number of boosting rounds\n",
    "    #model = xgb.train(params, dtrain, num_rounds)\n",
    "    ff_concept = nn.Sequential(nn.Linear(embedding_size, n_concepts), nn.Sigmoid()).to(device)\n",
    "    if backbone=='bert':\n",
    "        concept_encoder = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        model = torch.nn.Sequential(concept_encoder, ff_concept)\n",
    "    else:\n",
    "        model = torch.nn.Sequential(ff_concept)\n",
    "elif configuration=='XGBoost_ct':\n",
    "    # Set XGBoost parameters\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',  # Multiclass classification\n",
    "        'num_class': n_labels,                 # Number of classes\n",
    "        'max_depth': 3,                 # Maximum depth of each tree\n",
    "        'eta': 0.3,                     # Learning rate\n",
    "        'eval_metric': 'merror'         # Evaluation metric\n",
    "    }\n",
    "    # Train the XGBoost model\n",
    "    num_rounds = 5  # Number of boosting rounds\n",
    "    #model = xgb.train(params, dtrain, num_rounds)\n",
    "elif configuration=='e2e':\n",
    "    ff_task = nn.Sequential(nn.Linear(embedding_size, 100), nn.ReLU(), nn.Linear(100, n_labels), nn.Softmax()).to(device)\n",
    "    if backbone=='bert':\n",
    "        concept_encoder = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        model = torch.nn.Sequential(concept_encoder, ff_task)\n",
    "    else:\n",
    "        model = nn.Sequential(ff_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ecf179-0696-41c4-953b-035c31731169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if backbone=='bert':\n",
    "    for param in concept_encoder.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in concept_encoder.bert.encoder.layer[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "if configuration not in ['DTree_ct', 'XGBoost_ct']:\n",
    "    print('Number of trainable parameters:', sum(p.numel() if p.requires_grad==True else 0 for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b97717-8f22-46f8-91f6-dbeee495b667",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a982be-5eae-4846-872d-4be708b62caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='cebab':\n",
    "    concept_names = ['good food', 'good ambiance', 'good service', 'good noise']\n",
    "    class_names = ['bad', 'good']\n",
    "elif dataset=='drug':\n",
    "    concept_names = ['effectiveness', 'side effects']\n",
    "    class_names = ['0', '1', '2']\n",
    "elif dataset=='emo':\n",
    "    concept_names = ['joy', 'trust', 'sadness', 'surprise']\n",
    "    class_names = ['0', '1', '2']\n",
    "elif dataset=='depression':\n",
    "    concept_names = ['Deprecation', 'Loss_of_Interest', 'Hopelessness', 'Sleep_Disturbances', 'Appetite_Changes', 'Fatigue']\n",
    "    class_names = ['not_depressed', 'depressed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6a5b19-df44-41b0-8d66-6f187a150c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(loaded_set, rules=False):\n",
    "    model.eval()\n",
    "    running_task_loss = 0\n",
    "    running_concept_loss = 0\n",
    "    task_preds = torch.zeros(1,n_labels).to(device)\n",
    "    concept_preds =  torch.zeros(1,n_concepts).to(device)\n",
    "    pred_rules = []\n",
    "    original_sentences = torch.zeros(1,max_length).to(device)\n",
    "    real_labels = torch.zeros(1)\n",
    "    \n",
    "    for sentence_batch in loaded_set:\n",
    "        y = torch.Tensor(sentence_batch['labels'])\n",
    "        \n",
    "        # dummy tensor\n",
    "        target = torch.nn.functional.one_hot(y.to(torch.long), n_labels).to(device).to(torch.float)\n",
    "            \n",
    "        # true_concepts = torch.where(sentence_batch['concept_score']>0.5, 1, 0).to(device).to(torch.float)\n",
    "        if pseudo_labeling=='sbert':\n",
    "            if dataset=='cebab':\n",
    "                true_concepts = torch.where(F.softmax(torch.where(sentence_batch['concept_score']>threshold,sentence_batch['concept_score'],0), dim=-1)[:,:,1]>0.5, 1, 0).to(torch.float).to(device)\n",
    "            elif dataset=='drug':\n",
    "                true_concepts = torch.argmax(sentence_batch['concept_score'], dim=-1).to(torch.float).to(device) \n",
    "            elif dataset=='emo':\n",
    "                true_concepts = torch.where(sentence_batch['concept_score'][:,:,1]>threshold, 1, 0).to(torch.float).to(device)\n",
    "        elif pseudo_labeling in ['mistral', 'mixtral']:\n",
    "            true_concepts = sentence_batch['concept_score'].to(torch.float).to(device)\n",
    "            \n",
    "        if backbone=='bert':\n",
    "            emb = concept_encoder(sentence_batch['input_ids'].squeeze().to(torch.long).to(device), \n",
    "                                    sentence_batch['attention_mask'].squeeze().to(torch.long).to(device), \n",
    "                                    output_hidden_states=True).hidden_states[-1][:,0,:]\n",
    "        elif backbone=='mixtral':\n",
    "            emb = sentence_batch['embedded_reviews'].to(device)\n",
    "            \n",
    "        if dataset=='emo':\n",
    "            concept_labels = torch.cat([sentence_batch['joy'].unsqueeze(1), \n",
    "                                        sentence_batch['trust'].unsqueeze(1), \n",
    "                                        sentence_batch['sadness'].unsqueeze(1), \n",
    "                                        sentence_batch['surprise'].unsqueeze(1)], axis=1).to(device)\n",
    "        elif dataset=='cebab':\n",
    "            concept_labels = torch.cat([sentence_batch['food'].unsqueeze(1), sentence_batch['ambiance'].unsqueeze(1), \n",
    "                                        sentence_batch['service'].unsqueeze(1), sentence_batch['noise'].unsqueeze(1)], axis=1).to(device)\n",
    "        elif dataset=='drug':\n",
    "            concept_labels = torch.cat([sentence_batch['effectiveness'].unsqueeze(1), \n",
    "                                        sentence_batch['sideEffects'].unsqueeze(1)], axis=1).to(device)            \n",
    "\n",
    "        if configuration in ['dcr', 'supervised_dcr']:\n",
    "            c_emb, c_pred = cem(emb)\n",
    "            y_pred = dcr(c_emb, c_pred)\n",
    "            if configuration=='dcr':\n",
    "                running_concept_loss += loss_form(c_pred, true_concepts)\n",
    "            else:\n",
    "                running_concept_loss += loss_form(c_pred, concept_labels)\n",
    "            concept_preds = torch.cat([concept_preds, c_pred])\n",
    "            if rules:\n",
    "                local_explanations = dcr.explain(c_emb, c_pred, 'local', \n",
    "                                                 concept_names=concept_names, class_names=class_names)\n",
    "                pred_rules += local_explanations\n",
    "                original_sentences = torch.cat([original_sentences, sentence_batch['input_ids'].to(device)])\n",
    "        elif configuration in ['dcl', 'supervised_dcl']:\n",
    "            c_emb, c_pred = cem(emb)\n",
    "            y_pred = dcl(c_emb, c_pred)\n",
    "            if configuration=='dcl':\n",
    "                running_concept_loss += loss_form_concepts(c_pred, true_concepts)\n",
    "            else:\n",
    "                running_concept_loss += loss_form_concepts(c_pred, concept_labels)\n",
    "            concept_preds = torch.cat([concept_preds, c_pred])\n",
    "            if rules:\n",
    "                local_explanations = dcl.explain(c_emb, c_pred, 'local', \n",
    "                                                 concept_names=concept_names, class_names=class_names)\n",
    "                pred_rules += local_explanations\n",
    "                original_sentences = torch.cat([original_sentences, sentence_batch['input_ids'].to(device)])            \n",
    "        elif configuration=='cem':\n",
    "            c_emb, c_pred = cem(emb)\n",
    "            y_pred = ff(c_emb.flatten(start_dim=1))\n",
    "            running_concept_loss += loss_form(c_pred, true_concepts)\n",
    "            concept_preds = torch.cat([concept_preds, c_pred])\n",
    "        elif configuration=='cbm_fc':\n",
    "            c_pred = ff_concept(emb)\n",
    "            y_pred = fc_task(c_pred)\n",
    "            running_concept_loss += loss_form(c_pred, true_concepts)\n",
    "            concept_preds = torch.cat([concept_preds, c_pred]) \n",
    "        elif configuration in ['DTree', 'XGBoost']:\n",
    "            c_pred = ff_concept(emb)\n",
    "            concept_loss = loss_form(c_pred, true_concepts)\n",
    "            y_pred = torch.zeros(emb.shape[0], n_labels).to(device)\n",
    "            task_loss = torch.Tensor([0]).to(device)\n",
    "            running_task_loss += task_loss.item()\n",
    "            running_concept_loss += concept_loss\n",
    "            loss = concept_loss\n",
    "        elif configuration=='cbm_ff':\n",
    "            c_pred = ff_concept(emb)\n",
    "            y_pred = ff_task(c_pred)\n",
    "            running_concept_loss += loss_form(c_pred, true_concepts)\n",
    "            concept_preds = torch.cat([concept_preds, c_pred])\n",
    "        elif configuration=='modified_cem':\n",
    "            c_emb = cem(emb, true_concepts)\n",
    "            y_pred = dcr(c_emb, true_concepts)\n",
    "            running_concept_loss = torch.Tensor([0])\n",
    "            concept_preds = torch.cat([concept_preds, true_concepts])\n",
    "            if rules:\n",
    "                local_explanations = dcr.explain(c_emb, true_concepts, 'local', \n",
    "                                                 concept_names=concept_names, class_names=class_names)\n",
    "                pred_rules += local_explanations\n",
    "                original_sentences = torch.cat([original_sentences, sentence_batch['input_ids'].to(device)])\n",
    "        elif configuration=='cem_ct':\n",
    "            c_emb = cem(emb, true_concepts)\n",
    "            y_pred = ff(c_emb.flatten(start_dim=1))\n",
    "            running_concept_loss = torch.Tensor([0])\n",
    "            concept_preds = torch.cat([concept_preds, true_concepts])          \n",
    "        elif configuration=='modified_dcl':\n",
    "            c_emb = cem(emb, true_concepts)\n",
    "            y_pred = dcl(c_emb, true_concepts)\n",
    "            # task_loss = loss_form(y_pred, target)    \n",
    "            running_concept_loss = torch.Tensor([0])\n",
    "            concept_preds = torch.cat([concept_preds, true_concepts])\n",
    "            if rules:\n",
    "                local_explanations = dcl.explain(c_emb, true_concepts, 'local', \n",
    "                                                 concept_names=concept_names, class_names=class_names)\n",
    "                pred_rules += local_explanations\n",
    "                original_sentences = torch.cat([original_sentences, sentence_batch['input_ids'].to(device)])\n",
    "        elif configuration=='cbm_fc_ct':\n",
    "            y_pred = fc_task(true_concepts)\n",
    "            running_concept_loss = torch.Tensor([0])\n",
    "            concept_preds = torch.cat([concept_preds, true_concepts])\n",
    "        elif configuration=='cbm_ff_ct':\n",
    "            y_pred = ff_task(true_concepts)\n",
    "            running_concept_loss = torch.Tensor([0])\n",
    "            concept_preds = torch.cat([concept_preds, true_concepts])\n",
    "        elif configuration=='e2e':\n",
    "            y_pred = ff_task(emb)\n",
    "            running_concept_loss = torch.Tensor([0])\n",
    "            concept_preds = torch.cat([concept_preds, true_concepts])\n",
    "            \n",
    "        running_task_loss += loss_form(y_pred, target)     \n",
    "        task_preds = torch.cat([task_preds, y_pred])\n",
    "        real_labels = torch.cat([real_labels, y])\n",
    "    model.train()\n",
    "    \n",
    "    if rules:\n",
    "        return pred_rules, original_sentences[1:,:], real_labels[1:]\n",
    "    else:\n",
    "        return running_task_loss.item()/len(loaded_set), running_concept_loss.item()/len(loaded_set), task_preds[1:,:], concept_preds[1:,:], real_labels[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5e145-4362-460f-ba8b-1cb388f45780",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if configuration not in ['DTree_ct', 'XGBoost_ct']:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    if configuration in ['dcl', 'modified_dcl', 'supervised_dcl']:\n",
    "        loss_form = torch.nn.BCEWithLogitsLoss()\n",
    "        loss_form_concepts =  torch.nn.BCELoss()\n",
    "    else:\n",
    "        loss_form =  torch.nn.BCELoss()\n",
    "    train_task_losses = []\n",
    "    train_concept_losses = []\n",
    "    val_task_losses = []\n",
    "    val_concept_losses = []\n",
    "    epochs = args.epochs\n",
    "    scheduler = StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_task_loss = 0\n",
    "        running_concept_loss = 0\n",
    "        for sentence_batch in tqdm(loaded_train):\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y = torch.Tensor(sentence_batch['labels'])\n",
    "            # dummy tensor\n",
    "            target = torch.nn.functional.one_hot(y.to(torch.long), n_labels).to(device).to(torch.float)\n",
    "\n",
    "            # true_concepts = torch.where(sentence_batch['concept_score']>0.5, 1, 0).to(device).to(torch.float)\n",
    "            if pseudo_labeling=='sbert':\n",
    "                if dataset=='cebab':\n",
    "                    true_concepts = torch.where(F.softmax(torch.where(sentence_batch['concept_score']>threshold,sentence_batch['concept_score'],0), dim=-1)[:,:,1]>0.5, 1, 0).to(torch.float).to(device)\n",
    "                elif dataset=='drug':\n",
    "                    true_concepts = torch.argmax(sentence_batch['concept_score'], dim=-1).to(torch.float).to(device) #torch.where(F.softmax(sentence_batch['concept_score'], dim=-1)[:,:,1]>threshold, 1, 0).to(torch.float).to(device)\n",
    "                elif dataset=='emo':\n",
    "                    true_concepts = torch.where(sentence_batch['concept_score'][:,:,1]>threshold, 1, 0).to(torch.float).to(device)\n",
    "            elif pseudo_labeling in ['mistral', 'mixtral']:\n",
    "                true_concepts = sentence_batch['concept_score'].to(torch.float).to(device)\n",
    "\n",
    "            if backbone=='bert':\n",
    "                emb = concept_encoder(sentence_batch['input_ids'].squeeze().to(torch.long).to(device), \n",
    "                                        sentence_batch['attention_mask'].squeeze().to(torch.long).to(device), \n",
    "                                        output_hidden_states=True).hidden_states[-1][:,0,:]\n",
    "            elif backbone=='mixtral':\n",
    "                emb = sentence_batch['embedded_reviews'].to(device)\n",
    "\n",
    "            if dataset=='emo':\n",
    "                concept_labels = torch.cat([sentence_batch['joy'].unsqueeze(1), \n",
    "                                            sentence_batch['trust'].unsqueeze(1), \n",
    "                                            sentence_batch['sadness'].unsqueeze(1), \n",
    "                                            sentence_batch['surprise'].unsqueeze(1)], axis=1).to(device)\n",
    "            elif dataset=='cebab':\n",
    "                concept_labels = torch.cat([sentence_batch['food'].unsqueeze(1), sentence_batch['ambiance'].unsqueeze(1), \n",
    "                                            sentence_batch['service'].unsqueeze(1), sentence_batch['noise'].unsqueeze(1)], axis=1).to(device)\n",
    "            elif dataset=='drug':\n",
    "                concept_labels = torch.cat([sentence_batch['effectiveness'].unsqueeze(1), \n",
    "                                            sentence_batch['sideEffects'].unsqueeze(1)], axis=1).to(device)               \n",
    "\n",
    "            if configuration in ['dcr','supervised_dcr']:\n",
    "                c_emb, c_pred = cem(emb)\n",
    "                y_pred = dcr(c_emb, c_pred)\n",
    "                if configuration=='dcr':\n",
    "                    concept_loss = loss_form(c_pred, true_concepts)\n",
    "                else:\n",
    "                    concept_loss = loss_form(c_pred, concept_labels)\n",
    "                task_loss = loss_form(y_pred, target)    \n",
    "                running_task_loss += task_loss.item()\n",
    "                running_concept_loss += concept_loss.item()\n",
    "                loss = concept_loss + lambda_coeff * task_loss\n",
    "            if configuration in ['dcl', 'supervised_dcl']:\n",
    "                c_emb, c_pred = cem(emb)\n",
    "                y_pred, weight_attn, bias_attn = dcl(c_emb, c_pred, return_attn=True)\n",
    "                if configuration=='dcl':\n",
    "                    concept_loss = loss_form_concepts(c_pred, true_concepts)\n",
    "                else:\n",
    "                    concept_loss = loss_form_concepts(c_pred, concept_labels)\n",
    "                task_loss = loss_form(y_pred, target)    \n",
    "                running_task_loss += task_loss.item()\n",
    "                running_concept_loss += concept_loss.item()\n",
    "                #weight_attn_reg = dcl.entropy_reg(weight_attn)\n",
    "                penalty = 1e-6\n",
    "                loss = concept_loss + lambda_coeff * task_loss + penalty * torch.mean(weight_attn.abs()) + penalty * torch.mean(bias_attn.abs()**2)\n",
    "            elif configuration=='cem':\n",
    "                c_emb, c_pred = cem(emb)\n",
    "                y_pred = ff(c_emb.flatten(start_dim=1))\n",
    "                concept_loss = loss_form(c_pred, true_concepts)\n",
    "                task_loss = loss_form(y_pred, target)\n",
    "                running_task_loss += task_loss.item()\n",
    "                running_concept_loss += concept_loss.item()\n",
    "                loss = concept_loss + lambda_coeff * task_loss\n",
    "            elif configuration=='cbm_fc':\n",
    "                c_pred = ff_concept(emb)\n",
    "                y_pred = fc_task(c_pred)\n",
    "                concept_loss = loss_form(c_pred, true_concepts)\n",
    "                task_loss = loss_form(y_pred, target) \n",
    "                running_task_loss += task_loss.item()\n",
    "                running_concept_loss += concept_loss.item()\n",
    "                loss = concept_loss + lambda_coeff*task_loss\n",
    "            elif configuration in ['DTree', 'XGBoost']:\n",
    "                c_pred = ff_concept(emb)\n",
    "                concept_loss = loss_form(c_pred, true_concepts)\n",
    "                task_loss = torch.Tensor([0]).to(device)\n",
    "                running_task_loss += task_loss.item()\n",
    "                running_concept_loss += concept_loss.item()\n",
    "                loss = concept_loss\n",
    "            elif configuration=='cbm_ff':\n",
    "                c_pred = ff_concept(emb)\n",
    "                y_pred = ff_task(c_pred)\n",
    "                concept_loss = loss_form(c_pred, true_concepts)\n",
    "                task_loss = loss_form(y_pred, target) \n",
    "                running_task_loss += task_loss.item()\n",
    "                running_concept_loss += concept_loss.item()\n",
    "                loss = concept_loss + lambda_coeff*task_loss\n",
    "            elif configuration=='modified_cem':\n",
    "                c_emb = cem(emb, true_concepts)\n",
    "                y_pred = dcr(c_emb, true_concepts)\n",
    "                task_loss = loss_form(y_pred, target)    \n",
    "                running_task_loss += task_loss.item()\n",
    "                loss = task_loss\n",
    "            elif configuration=='cem_ct':\n",
    "                c_emb = cem(emb, true_concepts)\n",
    "                y_pred = ff(c_emb.flatten(start_dim=1))\n",
    "                task_loss = loss_form(y_pred, target)    \n",
    "                running_task_loss += task_loss.item()\n",
    "                loss = task_loss\n",
    "            elif configuration=='modified_dcl':\n",
    "                c_emb = cem(emb, true_concepts)\n",
    "                y_pred, weight_attn, bias_attn = dcl(c_emb, true_concepts, return_attn=True)\n",
    "                task_loss = loss_form(y_pred, target)    \n",
    "                running_task_loss += task_loss.item()\n",
    "                weight_attn_reg = dcl.entropy_reg(weight_attn)\n",
    "                penalty = 1e-6\n",
    "                loss = task_loss + penalty * torch.mean(weight_attn.abs()) + penalty * torch.mean(bias_attn.abs()**2)\n",
    "            elif configuration=='cbm_fc_ct':\n",
    "                y_pred = fc_task(true_concepts)\n",
    "                task_loss = loss_form(y_pred, target)    \n",
    "                running_task_loss += task_loss.item()\n",
    "                loss = task_loss\n",
    "            elif configuration=='cbm_ff_ct':\n",
    "                y_pred = ff_task(true_concepts)\n",
    "                task_loss = loss_form(y_pred, target)    \n",
    "                running_task_loss += task_loss.item()\n",
    "                loss = task_loss\n",
    "            elif configuration=='e2e':\n",
    "                y_pred = ff_task(emb)\n",
    "                task_loss = loss_form(y_pred, target)    \n",
    "                running_task_loss += task_loss.item()\n",
    "                loss = task_loss\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_task_losses.append(running_task_loss/len(loaded_train))\n",
    "        train_concept_losses.append(running_concept_loss/len(loaded_train))\n",
    "        val_task_loss, val_concept_loss, _, _, _ = evaluate(loaded_val)\n",
    "        val_task_losses.append(val_task_loss)\n",
    "        val_concept_losses.append(val_concept_loss)\n",
    "        \n",
    "    # Plot training curves\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    fig.set_size_inches(20,5)\n",
    "    ax[0].plot(range(1,epochs+1), train_task_losses, label='Training')\n",
    "    ax[0].plot(range(1,epochs+1), val_task_losses, label='Validation')\n",
    "    ax[0].grid()\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].set_ylabel(\"Loss\")\n",
    "    ax[0].set_title('Task Loss')\n",
    "    ax[0].legend()\n",
    "    ax[1].plot(range(1,epochs+1), train_concept_losses, label='Training')\n",
    "    ax[1].plot(range(1,epochs+1), val_concept_losses, label='Validation')\n",
    "    ax[1].grid()\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].set_ylabel(\"Loss\")\n",
    "    ax[1].set_title('Concept Loss')\n",
    "    ax[1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(result_folder+'training_plots.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Creating a DataFrame from lists\n",
    "    df = pd.DataFrame({'epoch': range(epochs), \n",
    "                      'train_task_losses': train_task_losses,\n",
    "                      'val_task_losses': val_task_losses,\n",
    "                      'train_concept_losses': train_concept_losses,\n",
    "                      'val_concept_losses': val_concept_losses})\n",
    "\n",
    "    # Name of the CSV file\n",
    "    csv_file = \"training_information.csv\"\n",
    "\n",
    "    # Writing DataFrame to CSV file\n",
    "    df.to_csv(result_folder+csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d099e03-36be-4b4e-99a0-6922a8b994ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if configuration in ['DTree_ct', 'DTree', 'XGBoost_ct', 'XGBoost']:\n",
    "    '''\n",
    "    if 'ct' not in configuration:      \n",
    "        # The concept predictor is loaded\n",
    "        concept_predictor_folder = [x for x in os.listdir(f\"results/{dataset}/{backbone}/\") if 'cbm_ff' in x and 'cbm_ff_ct' not in x][1]\n",
    "        print(concept_predictor_folder)\n",
    "        if backbone=='bert':\n",
    "            ff_concept = torch.load(f\"results/{dataset}/{backbone}/{concept_predictor_folder}/models/ff_concept\").cpu()\n",
    "            concept_encoder = torch.load(f\"results/{dataset}/{backbone}/{concept_predictor_folder}/models/concept_encoder\").cpu()\n",
    "        else:\n",
    "            ff_concept = torch.load(f\"results/{dataset}/{backbone}/{concept_predictor_folder}/models/ff_concept\").cpu()\n",
    "    '''\n",
    "    \n",
    "    if backbone=='bert':\n",
    "        for p in concept_encoder.parameters():\n",
    "            p.requires_grad=False\n",
    "            \n",
    "    concept_preds =  torch.zeros(1,n_concepts).to(device)\n",
    "    real_labels = torch.zeros(1)\n",
    "    for sentence_batch in tqdm(loaded_train):\n",
    "        y = torch.Tensor(sentence_batch['labels'])\n",
    "        # dummy tensor             \n",
    "        target = torch.nn.functional.one_hot(y.to(torch.long), n_labels).to(torch.float)\n",
    "        if 'ct' not in configuration:  \n",
    "            if backbone=='bert':\n",
    "                inputs = sentence_batch['input_ids'].squeeze().to(torch.long).to(device)\n",
    "                att_mask = sentence_batch['attention_mask'].squeeze().to(torch.long).to(device)\n",
    "                emb = concept_encoder(inputs, att_mask, output_hidden_states=True).hidden_states[-1][:,0,:]\n",
    "                c_pred = ff_concept(emb)   \n",
    "                del inputs\n",
    "                del att_mask\n",
    "                del emb\n",
    "            else:\n",
    "                embs = sentence_batch['embedded_reviews'].to(device)\n",
    "                c_pred = ff_concept(embs)\n",
    "                del embs\n",
    "            concept_preds = torch.cat([concept_preds, c_pred])\n",
    "        else:\n",
    "            concept_preds = torch.cat([concept_preds, sentence_batch['concept_score'].to(device)])\n",
    "        real_labels = torch.cat([real_labels, y])\n",
    "    if 'cuda' in concept_preds.device.type:\n",
    "        concept_preds = concept_preds.cpu()\n",
    "    concept_preds = concept_preds[1:,:].detach().numpy()\n",
    "    real_labels = real_labels[1:].numpy()\n",
    "\n",
    "    if 'DTree' in configuration:\n",
    "        dtree.fit(concept_preds, real_labels)\n",
    "    elif 'XGBoost' in configuration:\n",
    "        dtrain = xgb.DMatrix(concept_preds, label=real_labels)\n",
    "        dtree = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "    concept_preds =  torch.zeros(1,n_concepts).to(device)\n",
    "    \n",
    "    y_true = torch.zeros(1)\n",
    "    for sentence_batch in tqdm(loaded_test):\n",
    "        y = torch.Tensor(sentence_batch['labels'])\n",
    "        # dummy tensor             \n",
    "        if 'ct' not in configuration:      \n",
    "            if backbone=='bert':\n",
    "                emb = concept_encoder(sentence_batch['input_ids'].squeeze().to(torch.long).to(device), \n",
    "                                        sentence_batch['attention_mask'].squeeze().to(torch.long).to(device), \n",
    "                                        output_hidden_states=True).hidden_states[-1][:,0,:]\n",
    "                c_pred = ff_concept(emb)   \n",
    "            else:\n",
    "                c_pred = ff_concept(sentence_batch['embedded_reviews'].to(device))\n",
    "            concept_preds = torch.cat([concept_preds, c_pred])\n",
    "        else:\n",
    "            concept_preds = torch.cat([concept_preds, sentence_batch['concept_score'].to(device)])\n",
    "        y_true = torch.cat([y_true, y])\n",
    "    if 'cuda' in concept_preds.device.type:\n",
    "        concept_preds = concept_preds.cpu()\n",
    "    concept_preds = concept_preds[1:,:].detach().numpy()\n",
    "    y_true = y_true[1:].numpy()\n",
    "\n",
    "    if 'DTree' in configuration:\n",
    "        y_pred = dtree.predict(concept_preds)\n",
    "    elif 'XGBoost' in configuration:\n",
    "        dtest = xgb.DMatrix(concept_preds)\n",
    "        y_pred = dtree.predict(dtest)\n",
    "        \n",
    "    c_pred = torch.Tensor(concept_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cde17f7-9f72-411d-bbe3-84dc46889948",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a5bbe-87e1-481d-b614-a2ac8d0471d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'DTree' in configuration or 'XGBoost' in configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad688400-c60f-4186-9947-53d34097ff8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = result_folder + 'models/'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if backbone=='bert' and 'ct' not in configuration:\n",
    "    if 'DTree' in configuration or 'XGBoost' in configuration:\n",
    "        torch.save(ff_concept, model_path+'concept_encoder') \n",
    "    else:\n",
    "        torch.save(concept_encoder, model_path+'concept_encoder') \n",
    "\n",
    "if configuration in ['dcr', 'supervised_dcr']:\n",
    "    torch.save(cem, model_path+'cem')\n",
    "    torch.save(dcr, model_path+'dcr')\n",
    "elif configuration=='cem':\n",
    "    torch.save(cem, model_path+'cem')\n",
    "    torch.save(ff, model_path+'ff')\n",
    "elif configuration=='cem_ct':\n",
    "    torch.save(cem, model_path+'cem')\n",
    "    torch.save(ff, model_path+'ff')    \n",
    "elif configuration=='modified_cem':\n",
    "    torch.save(cem, model_path+'cem')\n",
    "    torch.save(dcr, model_path+'dcr')\n",
    "elif configuration=='cbm_fc':\n",
    "    torch.save(ff_concept, model_path+'ff_concept')\n",
    "    torch.save(fc_task, model_path+'fc_task')\n",
    "elif configuration=='cbm_ff':\n",
    "    torch.save(ff_concept, model_path+'ff_concept')\n",
    "    torch.save(ff_task, model_path+'ff_task')\n",
    "elif configuration=='cbm_fc_ct':\n",
    "    torch.save(fc_task, model_path+'fc_task')\n",
    "elif configuration=='cbm_ff_ct':\n",
    "    torch.save(ff_task, model_path+'ff_task')\n",
    "elif configuration in ['DTree', 'XGBoost', 'DTree_ct', 'XGBoost_ct']:\n",
    "    joblib.dump(dtree, model_path+'tree_based_model')\n",
    "elif configuration in ['dcl', 'supervised_dcl']:\n",
    "    torch.save(cem, model_path+'cem')\n",
    "    torch.save(dcl, model_path+'dcl')   \n",
    "elif configuration in ['modified_dcl']:\n",
    "    torch.save(cem, model_path+'cem')\n",
    "    torch.save(dcl, model_path+'dcl')\n",
    "elif configuration=='e2e':\n",
    "    torch.save(ff_task, model_path+'ff_task')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820bddc-dd67-43a5-b120-17c94d9df1a5",
   "metadata": {},
   "source": [
    "# Task results (test-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ce7b65-d139-496f-a4e1-0a4ad598fb4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if configuration not in ['DTree', 'XGBoost', 'DTree_ct', 'XGBoost_ct']:\n",
    "    _, _, y_pred, c_pred, y_true = evaluate(loaded_test)\n",
    "    y_pred = torch.argmax(y_pred, dim=-1).detach().cpu().numpy()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "n = len(os.listdir('results'))+1\n",
    "report_string = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "# Convert the report to a DataFrame\n",
    "report_df = pd.DataFrame(report_string).T\n",
    "if n_labels==2:\n",
    "    report_df.index = [\"0\", \"1\", \"accuracy\", \"macro avg\", \"weighted avg\"]\n",
    "else:\n",
    "    report_df.index = [\"0\", \"1\", \"2\", \"accuracy\", \"macro avg\", \"weighted avg\"]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "report_df.to_csv(result_folder+'classification_report.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c03b0-8832-496d-8b04-ccebd7ab61ab",
   "metadata": {},
   "source": [
    "## Concept results (test-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b8ecdd-0ccf-4210-80cf-6d070a2daab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset!='depression':\n",
    "    true_concepts = torch.zeros(1, n_concepts)\n",
    "    for sentence_batch in loaded_test:\n",
    "        if dataset=='cebab':\n",
    "            food = sentence_batch['food']\n",
    "            ambiance = sentence_batch['ambiance']\n",
    "            service = sentence_batch['service']\n",
    "            noise = sentence_batch['noise']\n",
    "            true_concepts = torch.cat([true_concepts, torch.hstack([food.unsqueeze(1), ambiance.unsqueeze(1), service.unsqueeze(1), noise.unsqueeze(1)])])  \n",
    "        elif dataset=='drug':\n",
    "            effectiveness = sentence_batch['effectiveness']\n",
    "            sideEffects = sentence_batch['sideEffects']\n",
    "            true_concepts = torch.cat([true_concepts, torch.hstack([effectiveness.unsqueeze(1), sideEffects.unsqueeze(1)])])  \n",
    "        elif dataset=='emo':\n",
    "            gioia = sentence_batch['joy']\n",
    "            fiducia = sentence_batch['trust']\n",
    "            tristezza = sentence_batch['sadness']\n",
    "            sorpresa = sentence_batch['surprise']\n",
    "            true_concepts = torch.cat([true_concepts, torch.hstack([gioia.unsqueeze(1), fiducia.unsqueeze(1), tristezza.unsqueeze(1), sorpresa.unsqueeze(1)])])         \n",
    "    true_concepts = true_concepts[1:,:]\n",
    "\n",
    "    fig, ax = plt.subplots(1, n_concepts)\n",
    "    fig.set_size_inches(20,5)\n",
    "\n",
    "    if not os.path.exists(result_folder+'concepts'):\n",
    "        os.makedirs(result_folder+'concepts')\n",
    "\n",
    "    for i in range(n_concepts):\n",
    "        cm = confusion_matrix(true_concepts[:,i], \n",
    "                              torch.where(c_pred[:,i]>0.5,1,0).detach().cpu().numpy())\n",
    "        sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', ax=ax[i])\n",
    "        ax[i].set_xlabel(\"Epochs\")\n",
    "        ax[i].set_ylabel(\"Loss\")\n",
    "\n",
    "        cr = pd.DataFrame(classification_report(true_concepts[:,i], torch.where(c_pred[:,i]>0.5,1,0).detach().cpu().numpy(), output_dict=True)).T\n",
    "        cr.index = [\"0\", \"1\", \"accuracy\", \"macro avg\", \"weighted avg\"]\n",
    "        cr.to_csv(result_folder+f'/concepts/{names[i]}_classification_report.csv', index=True)\n",
    "\n",
    "        ax[i].set_title(names[i]+'; f1-score macro: '+str(round(classification_report(true_concepts[:,i], torch.where(c_pred[:,i]>0.5,1,0).detach().cpu().numpy(), output_dict=True)['macro avg']['f1-score'],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f115b2f4-ebaf-4e16-8257-bcef9042f057",
   "metadata": {},
   "source": [
    "# Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119a31fa-9c34-4157-b11e-17ec2c37e276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "load_model = False\n",
    "if load_model:\n",
    "    configuration = 'dcr'\n",
    "    exp = '4'\n",
    "    result_folder = f'results/{dataset}/mixtral/{configuration}_E{exp}/'\n",
    "    print(result_folder)\n",
    "    model_path = result_folder + 'models/'\n",
    "    if configuration in ['supervised_dcr', 'dcr']:\n",
    "        #concept_encoder = torch.load(model_path+'concept_encoder')\n",
    "        cem = torch.load(model_path+'cem')\n",
    "        dcr = torch.load(model_path+'dcr')\n",
    "        model = nn.Sequential(cem, dcr)\n",
    "    elif configuration=='cem':\n",
    "        concept_encoder = torch.load(model_path+'concept_encoder')\n",
    "        cem = torch.load(model_path+'cem')\n",
    "        ff = torch.load(model_path+'ff')\n",
    "        model = nn.Sequential(concept_encoder, cem, ff)\n",
    "    elif configuration=='cbm_fc':\n",
    "        fc = torch.load(model_path+'fc')    \n",
    "        model = nn.Sequential(fc)\n",
    "    elif configuration=='modified_cem':\n",
    "        cem = torch.load(model_path+'cem')\n",
    "        dcr = torch.load(model_path+'dcr')\n",
    "        model = nn.Sequential(cem, dcr)\n",
    "    elif configuration in ['supervised_dcl', 'dcl']:\n",
    "        #concept_encoder = torch.load(model_path+'concept_encoder')\n",
    "        cem = torch.load(model_path+'cem')\n",
    "        dcl = torch.load(model_path+'dcl')\n",
    "        model = nn.Sequential(cem, dcl)   \n",
    "    elif configuration=='modified_dcl':\n",
    "        cem = torch.load(model_path+'cem')\n",
    "        dcl = torch.load(model_path+'dcl')\n",
    "        model = nn.Sequential(cem, dcl)\n",
    "    elif configuration=='cbm_ff':\n",
    "        ff = torch.load(model_path+'fc')\n",
    "        model = nn.Sequential(ff)\n",
    "    elif configuration=='cbm_fc_bert':\n",
    "        concept_encoder = torch.load(model_path+'concept_encoder')\n",
    "        ff_concept = torch.load(model_path+'ff_concept')\n",
    "        fc_task = torch.load(model_path+'fc_task')\n",
    "        model = nn.Sequential(concept_encoder, ff_concept, fc_task)\n",
    "    elif configuration=='cbm_ff_bert':\n",
    "        ff_concept = torch.load(model_path+'ff_concept')\n",
    "        concept_encoder = torch.load(model_path+'concept_encoder')\n",
    "        ff_task = torch.load(model_path+'ff_task')\n",
    "        model = nn.Sequential(concept_encoder, ff_concept, ff_task)\n",
    "        \n",
    "    if configuration in ['dcl', 'modified_dcl', 'supervised_dcl']:\n",
    "        loss_form = torch.nn.BCEWithLogitsLoss()\n",
    "        loss_form_concepts =  torch.nn.BCELoss()\n",
    "    else:\n",
    "        loss_form =  torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c3e3da-c7c4-418f-8a75-521237465a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rules, original, lab = evaluate(loaded_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b5db61-6059-4276-a05c-038497fc0dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rules_to_store = pd.DataFrame(columns=['text', 'true', 'prediction', 'rule'])\n",
    "for rule, tokens, l in zip(rules, original, lab):\n",
    "    d = {}\n",
    "    d['true'] = int(l)\n",
    "    d['text'] = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    d['prediction'] = rule['class']\n",
    "    if 'dcl' in configuration:\n",
    "        if rule['explanation']!='':\n",
    "            d['rule'] = {k:round(float(v),2) for k, v in rule['explanation'].items()}\n",
    "        else:\n",
    "            d['rule'] = ''\n",
    "    else:\n",
    "        d['rule'] = rule['explanation']\n",
    "    df = pd.DataFrame([d])\n",
    "    rules_to_store = pd.concat([rules_to_store, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897647e-e246-4b0c-a887-7012d93d3229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rules_to_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe2b81-ef40-44fb-8356-d5240dece40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_to_store.to_csv(result_folder+'rules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f2ec0-b7d7-4568-b65a-0f013ca94020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "4a253b64-400d-4d8f-b960-9ff8cf5d4b07",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
